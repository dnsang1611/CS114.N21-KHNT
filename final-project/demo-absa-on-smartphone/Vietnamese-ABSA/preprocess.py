import regex as re
import string
import emoji

from vncorenlp import VnCoreNLP
from nltk import flatten


# Xo√° c√°c tag HTML
def remove_HTML(text):
    return re.sub(r'<[^>]*>', '', text)


# Chu·∫©n ho√° unicode
def convert_unicode(text):
    char1252 = 'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£'
    charutf8 = '√†|√°|·∫£|√£|·∫°|·∫ß|·∫•|·∫©|·∫´|·∫≠|·∫±|·∫Ø|·∫≥|·∫µ|·∫∑|√®|√©|·∫ª|·∫Ω|·∫π|·ªÅ|·∫ø|·ªÉ|·ªÖ|·ªá|√¨|√≠|·ªâ|ƒ©|·ªã|√≤|√≥|·ªè|√µ|·ªç|·ªì|·ªë|·ªï|·ªó|·ªô|·ªù|·ªõ|·ªü|·ª°|·ª£|√π|√∫|·ªß|≈©|·ª•|·ª´|·ª©|·ª≠|·ªØ|·ª±|·ª≥|√Ω|·ª∑|·ªπ|·ªµ|√Ä|√Å|·∫¢|√É|·∫†|·∫¶|·∫§|·∫®|·∫™|·∫¨|·∫∞|·∫Æ|·∫≤|·∫¥|·∫∂|√à|√â|·∫∫|·∫º|·∫∏|·ªÄ|·∫æ|·ªÇ|·ªÑ|·ªÜ|√å|√ç|·ªà|ƒ®|·ªä|√í|√ì|·ªé|√ï|·ªå|·ªí|·ªê|·ªî|·ªñ|·ªò|·ªú|·ªö|·ªû|·ª†|·ª¢|√ô|√ö|·ª¶|≈®|·ª§|·ª™|·ª®|·ª¨|·ªÆ|·ª∞|·ª≤|√ù|·ª∂|·ª∏|·ª¥'
    char1252 = char1252.split('|')
    charutf8 = charutf8.split('|')

    dic = {}
    for i in range(len(char1252)): dic[char1252[i]] = charutf8[i]
    return re.sub(
        r'aÃÄ|aÃÅ|aÃâ|aÃÉ|aÃ£|√¢ÃÄ|√¢ÃÅ|√¢Ãâ|√¢ÃÉ|√¢Ã£|ƒÉÃÄ|ƒÉÃÅ|ƒÉÃâ|ƒÉÃÉ|ƒÉÃ£|eÃÄ|eÃÅ|eÃâ|eÃÉ|eÃ£|√™ÃÄ|√™ÃÅ|√™Ãâ|√™ÃÉ|√™Ã£|iÃÄ|iÃÅ|iÃâ|iÃÉ|iÃ£|oÃÄ|oÃÅ|oÃâ|oÃÉ|oÃ£|√¥ÃÄ|√¥ÃÅ|√¥Ãâ|√¥ÃÉ|√¥Ã£|∆°ÃÄ|∆°ÃÅ|∆°Ãâ|∆°ÃÉ|∆°Ã£|uÃÄ|uÃÅ|uÃâ|uÃÉ|uÃ£|∆∞ÃÄ|∆∞ÃÅ|∆∞Ãâ|∆∞ÃÉ|∆∞Ã£|yÃÄ|yÃÅ|yÃâ|yÃÉ|yÃ£|AÃÄ|AÃÅ|AÃâ|AÃÉ|AÃ£|√ÇÃÄ|√ÇÃÅ|√ÇÃâ|√ÇÃÉ|√ÇÃ£|ƒÇÃÄ|ƒÇÃÅ|ƒÇÃâ|ƒÇÃÉ|ƒÇÃ£|EÃÄ|EÃÅ|EÃâ|EÃÉ|EÃ£|√äÃÄ|√äÃÅ|√äÃâ|√äÃÉ|√äÃ£|IÃÄ|IÃÅ|IÃâ|IÃÉ|IÃ£|OÃÄ|OÃÅ|OÃâ|OÃÉ|OÃ£|√îÃÄ|√îÃÅ|√îÃâ|√îÃÉ|√îÃ£|∆†ÃÄ|∆†ÃÅ|∆†Ãâ|∆†ÃÉ|∆†Ã£|UÃÄ|UÃÅ|UÃâ|UÃÉ|UÃ£|∆ØÃÄ|∆ØÃÅ|∆ØÃâ|∆ØÃÉ|∆ØÃ£|YÃÄ|YÃÅ|YÃâ|YÃÉ|YÃ£',
        lambda x: dic[x.group()], text
    )


# Standardize accent typing
vowels_to_ids = {}
vowels_table = [
    ['a', '√†', '√°', '·∫£', '√£', '·∫°', 'a' ],
    ['ƒÉ', '·∫±', '·∫Ø', '·∫≥', '·∫µ', '·∫∑', 'aw'],
    ['√¢', '·∫ß', '·∫•', '·∫©', '·∫´', '·∫≠', 'aa'],
    ['e', '√®', '√©', '·∫ª', '·∫Ω', '·∫π', 'e' ],
    ['√™', '·ªÅ', '·∫ø', '·ªÉ', '·ªÖ', '·ªá', 'ee'],
    ['i', '√¨', '√≠', '·ªâ', 'ƒ©', '·ªã', 'i' ],
    ['o', '√≤', '√≥', '·ªè', '√µ', '·ªç', 'o' ],
    ['√¥', '·ªì', '·ªë', '·ªï', '·ªó', '·ªô', 'oo'],
    ['∆°', '·ªù', '·ªõ', '·ªü', '·ª°', '·ª£', 'ow'],
    ['u', '√π', '√∫', '·ªß', '≈©', '·ª•', 'u' ],
    ['∆∞', '·ª´', '·ª©', '·ª≠', '·ªØ', '·ª±', 'uw'],
    ['y', '·ª≥', '√Ω', '·ª∑', '·ªπ', '·ªµ', 'y' ]
]

for i in range(len(vowels_table)):
    for j in range(len(vowels_table[i]) - 1):
        vowels_to_ids[vowels_table[i][j]] = (i, j)


def is_valid_vietnamese_word(word):
    # Ki·ªÉm tra xem c√°c k√Ω t·ª± nguy√™n √¢m c√≥ li·ªÅn nhau hay kh√¥ng
    chars = list(word)
    vowel_indexes = -1
    for index, char in enumerate(chars):
        x, y = vowels_to_ids.get(char, (-1, -1))
        if x != -1:
            if vowel_indexes == -1: vowel_indexes = index
            else:
                if index - vowel_indexes != 1: return False
                vowel_indexes = index
    return True


def standardize_word_typing(word):
    '''
    Chu·∫©n ho√° c√°c nguy√™n √¢m trong ti·∫øng vi·ªát d·ª±a tr√™n nguy√™n
    t·∫Øc ƒë·∫∑t d·∫•u thanh trong ch·ªØ qu·ªëc ng·ªØ (ki·ªÉu c≈©)
    ==> Vi·ªác n√†y gi√∫p gi·∫£m thi·ªÉu c√°c t·ª´ trong t·ª´ ƒëi·ªÉn
    ==> TƒÉng performance.

    VD: g·ªâa, gi·∫£ ==> ƒê∆∞a v·ªÅ gi·∫£

    Tham kh·∫£o: https://vi.wikipedia.org/wiki/Quy_t%E1%BA%AFc_%C4%91%E1%BA%B7t_d%E1%BA%A5u_thanh_trong_ch%E1%BB%AF_qu%E1%BB%91c_ng%E1%BB%AF
    '''

    # N·∫øu kh√¥ng ph·∫£i t·ª´ ti·∫øng vi·ªát h·ª£p l·ªá th√¨ ta kh√¥ng c·∫ßn chu·∫©n ho√° c√°c nguy√™n √¢m
    if not is_valid_vietnamese_word(word): return word
    chars = list(word)
    dau_cau = 0
    vowel_indexes = []
    qu_or_gi = False

    # Trong ti·∫øng vi·ªát c√≥ 2 ph·ª• √¢m k√©p c√≥ k√Ω t·ª± th√†nh ph·∫ßn gi·ªëng nguy√™n √¢m
    # l√† qu v√† gi, v√¨ v·∫≠y ta c·∫ßn tr√°nh nh·∫ßm l·∫´n c√°c k√Ω t·ª± u, i trong
    # qu v√† gi l√† nguy√™n √¢m.
    for index, char in enumerate(chars):
        x, y = vowels_to_ids.get(char, (-1, -1))
        if x == -1: continue
        elif x == 9:  # check qu
            if index != 0 and chars[index - 1] == 'q':
                chars[index] = 'u'
                qu_or_gi = True
        elif x == 5:  # check gi
            if index != 0 and chars[index - 1] == 'g':
                chars[index] = 'i'
                qu_or_gi = True

        # N·∫øu y != 0, th√¨ t·ª´ s·∫Ω c√≥ d·∫•u c√¢u (h·ªèi, ng√£, n·∫∑ng, ...)
        if y != 0:
            dau_cau = y
            chars[index] = vowels_table[x][0]

        # N·∫øu k√Ω t·ª± hi·ªán t·∫°i n·∫±m trong b·∫£ng nguy√™n √¢m
        # v√† kh√¥ng ph·∫£i l√† k√Ω t·ª± trong ph·ª• √¢m qu, gi th√¨ ta
        # s·∫Ω ch√®n v√†o vowel_indexes
        if not qu_or_gi or index != 1:
            vowel_indexes.append(index)

    # Tr∆∞·ªùng h·ª£p t·ª´ c√≥ 0 ho·∫∑c 1 nguy√™n √¢m
    if len(vowel_indexes) < 2:
        # V√¨ d·∫•u thanh ch·ªâ c√≥ kh·∫£ nƒÉng tr√™n nguy√™n √¢m, n√™n c√°c tr∆∞·ªùng h·ª£p ph·ª• √¢m
        # kh√¥ng ph·∫£i qu ho·∫∑c gi ta ch·ªâ c·∫ßn tr·∫£ v·ªÅ t·ª´ g·ªëc.
        if qu_or_gi:
            # V√≠ d·ª•: g·ªâ sau ƒë·ª£t x·ª≠ l√Ω tr√™n s·∫Ω ƒë∆∞·ª£c t√°ch th√†nh list chas ['g', 'i']
            # v√† dau_cau = 3 (d·∫•u h·ªèi) th√¨ ta s·∫Ω tr·∫£ l·∫°i ƒë√∫ng th√†nh g·ªâ
            if len(chars) == 2:
                x, y = vowels_to_ids.get(chars[1])
                chars[1] = vowels_table[x][dau_cau]

            # N·∫øu c√≥ 1 nguy√™n √¢m, th√¨ nguy√™n √¢m n√†y s·∫Ω ƒë∆∞·ª£c tr·∫£ v·ªÅ ƒë√∫ng d·∫•u thanh ban ƒë·∫ßu.
            # VD: "gi·∫£" ho·∫∑c "g·ªâa" ==> ['g', 'i', 'a'] v√† d·∫•u c√¢u l√† h·ªèi ==> gi·∫£
            else:
                x, y = vowels_to_ids.get(chars[2], (-1, -1))
                if x != -1: chars[2] = vowels_table[x][dau_cau]
                else: chars[1] = vowels_table[5][dau_cau] if chars[1] == 'i' else vowels_table[9][dau_cau]
            return ''.join(chars)
        return word

    # Ngo·∫°i l·ªá l√† ch·ªØ "√™" v√† "∆°" chi·∫øm ∆∞u ti√™n, b·∫•t k·ªÉ v·ªã tr√≠.
    for index in vowel_indexes:
        x, y = vowels_to_ids[chars[index]]
        if x == 4 or x == 8:  # √™, ∆°
            chars[index] = vowels_table[x][dau_cau]
            return ''.join(chars)

    # N·∫øu l√† t·∫≠p h·ª£p hai (2) nguy√™n √¢m (nguy√™n √¢m ƒë√¥i) th√¨ ƒë√°nh d·∫•u ·ªü nguy√™n √¢m ƒë·∫ßu.
    # T·∫≠p h·ª£p ba (3) nguy√™n √¢m (nguy√™n √¢m ba) ho·∫∑c hai nguy√™n √¢m + ph·ª• √¢m cu·ªëi
    # th√¨ v·ªã tr√≠ d·∫•u chuy·ªÉn ƒë·∫øn nguy√™n √¢m th·ª© nh√¨
    if len(vowel_indexes) == 2:
        if vowel_indexes[-1] == len(chars) - 1:
            x, y = vowels_to_ids[chars[vowel_indexes[0]]]
            chars[vowel_indexes[0]] = vowels_table[x][dau_cau]
        else:
            x, y = vowels_to_ids[chars[vowel_indexes[1]]]
            chars[vowel_indexes[1]] = vowels_table[x][dau_cau]
    else:
        x, y = vowels_to_ids[chars[vowel_indexes[1]]]
        chars[vowel_indexes[1]] = vowels_table[x][dau_cau]
    return ''.join(chars)


def standardize_sentence_typing(text):
    words = text.lower().split()
    for index, word in enumerate(words):
        # D√πng regex ƒë·ªÉ t√°ch th√†nh c·ª•m c√≥ d·∫°ng "<d·∫•u c√¢u>/<word>/<d·∫•u c√¢u>"
        cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
        if len(cw) == 3: cw[1] = standardize_word_typing(cw[1])
        words[index] = ''.join(cw)
    return ' '.join(words)


# Chu·∫©n ho√° t·ª´ vi·∫øt t·∫Øt
# !wget https://gist.githubusercontent.com/nguyenvanhieuvn/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt
replace_list = {
    '√¥ k√™i': 'ok', 'okie': 'ok', 'o k√™': 'ok', 'okey': 'ok', '√¥k√™': 'ok', 'oki': 'ok', 'oke': 'ok', 'okay': 'ok', 'ok√™': 'ok',
    'tks': 'c·∫£m ∆°n', 'thks': 'c·∫£m ∆°n', 'thanks': 'c·∫£m ∆°n', 'ths': 'c·∫£m ∆°n', 'thank': 'c·∫£m ∆°n',
    'kg': 'kh√¥ng', 'not': 'kh√¥ng', 'k': 'kh√¥ng', 'kh': 'kh√¥ng', 'k√¥': 'kh√¥ng', 'hok': 'kh√¥ng', 'ko': 'kh√¥ng', 'khong': 'kh√¥ng', 'kp': 'kh√¥ng ph·∫£i',
    'he he': 't√≠ch c·ª±c', 'hehe': 't√≠ch c·ª±c', 'hihi': 't√≠ch c·ª±c', 'haha': 't√≠ch c·ª±c', 'hjhj': 't√≠ch c·ª±c', 'thick': 't√≠ch c·ª±c',
    'lol': 'ti√™u c·ª±c', 'cc': 'ti√™u c·ª±c', 'huhu': 'ti√™u c·ª±c', 'cute': 'd·ªÖ th∆∞∆°ng',

    'sz': 'c·ª°', 'size': 'c·ª°',
    'wa': 'qu√°', 'w√°': 'qu√°', 'q√°': 'qu√°',
    'ƒëx': 'ƒë∆∞·ª£c', 'dk': 'ƒë∆∞·ª£c', 'dc': 'ƒë∆∞·ª£c', 'ƒëk': 'ƒë∆∞·ª£c', 'ƒëc': 'ƒë∆∞·ª£c',
    'vs': 'v·ªõi', 'j': 'g√¨', '‚Äú': ' ', 'time': 'th·ªùi gian', 'm': 'm√¨nh', 'mik': 'm√¨nh', 'r': 'r·ªìi', 'bjo': 'bao gi·ªù', 'very': 'r·∫•t',

    'authentic': 'chu·∫©n ch√≠nh h√£ng', 'aut': 'chu·∫©n ch√≠nh h√£ng', 'auth': 'chu·∫©n ch√≠nh h√£ng', 'date': 'h·∫°n s·ª≠ d·ª•ng', 'hsd': 'h·∫°n s·ª≠ d·ª•ng',
    'store': 'c·ª≠a h√†ng', 'sop': 'c·ª≠a h√†ng', 'shopE': 'c·ª≠a h√†ng', 'shop': 'c·ª≠a h√†ng',
    'sp': 's·∫£n ph·∫©m', 'product': 's·∫£n ph·∫©m', 'h√†g': 'h√†ng',
    'ship': 'giao h√†ng', 'delivery': 'giao h√†ng', 's√≠p': 'giao h√†ng', 'order': 'ƒë·∫∑t h√†ng',

    'gud': 't·ªët', 'wel done': 't·ªët', 'good': 't·ªët', 'g√∫t': 't·ªët', 'tot': 't·ªët', 'nice': 't·ªët', 'perfect': 'r·∫•t t·ªët',
    'quality': 'ch·∫•t l∆∞·ª£ng', 'ch·∫•t lg': 'ch·∫•t l∆∞·ª£ng', 'chat': 'ch·∫•t', 'excelent': 'ho√†n h·∫£o', 'bt': 'b√¨nh th∆∞·ªùng',
    'sad': 't·ªá', 'por': 't·ªá', 'poor': 't·ªá', 'bad': 't·ªá',
    'beautiful': 'ƒë·∫πp tuy·ªát v·ªùi', 'dep': 'ƒë·∫πp',
    'xau': 'x·∫•u', 's·∫•u': 'x·∫•u',

    'thik': 'th√≠ch', 'iu': 'y√™u', 'fake': 'gi·∫£ m·∫°o',
    'quickly': 'nhanh', 'quick': 'nhanh', 'fast': 'nhanh',
    'fresh': 't∆∞∆°i', 'delicious': 'ngon',

    'dt': 'ƒëi·ªán tho·∫°i', 'fb': 'facebook', 'face': 'facebook', 'ks': 'kh√°ch s·∫°n', 'nv': 'nh√¢n vi√™n',
    'nt': 'nh·∫Øn tin', 'ib': 'nh·∫Øn tin', 'tl': 'tr·∫£ l·ªùi', 'trl': 'tr·∫£ l·ªùi', 'rep': 'tr·∫£ l·ªùi',
    'fback': 'feedback', 'fedback': 'feedback',
    'sd': 's·ª≠ d·ª•ng', 's√†i': 'x√†i',

    'üòä': 't√≠ch c·ª±c', 'üôÇ': 't√≠ch c·ª±c', 'üôÅ': 'ti√™u c·ª±c',
    '‚ù§Ô∏è': 't√≠ch c·ª±c', 'üëç': 't√≠ch c·ª±c', 'üéâ': 't√≠ch c·ª±c', 'üòÄ': 't√≠ch c·ª±c', 'üòç': 't√≠ch c·ª±c', 'üòÇ': 't√≠ch c·ª±c', 'ü§ó': 't√≠ch c·ª±c', 'üòô': 't√≠ch c·ª±c', 'üôÇ': 't√≠ch c·ª±c',
    'üòî': 'ti√™u c·ª±c', 'üòì': 'ti√™u c·ª±c',
    '‚≠ê': 'star', '*': 'star', 'üåü': 'star'
}

with open('teencode.txt', encoding='utf-8') as f:
    for pair in f.readlines():
        key, value = pair.split('\t')
        replace_list[key] = value.strip()


def normalize_acronyms(text):
    words = []
    for word in text.strip().split():
        # word = word.strip(string.punctuation)
        if word.lower() not in replace_list.keys(): words.append(word)
        else: words.append(replace_list[word.lower()])
    return emoji.demojize(' '.join(words)) # Remove Emojis


# Word segmentation
annotator = VnCoreNLP('VnCoreNLP/VnCoreNLP-1.1.1.jar')
def word_segmentation(text):
    words = annotator.tokenize(text)
    return ' '.join(word for word in flatten(words))


# Xo√° c√°c k√Ω t·ª± kh√¥ng c·∫ßn thi·∫øt
def remove_unnecessary_characters(text):
    text = re.sub(r'[^\s\w√°√†·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√©√®·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√≥√≤·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√≠√¨·ªâƒ©·ªã√∫√π·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±√Ω·ª≥·ª∑·ªπ·ªµƒë√Å√Ä·∫¢√É·∫†ƒÇ·∫Æ·∫∞·∫≤·∫¥·∫∂√Ç·∫§·∫¶·∫®·∫™·∫¨√â√à·∫∫·∫º·∫∏√ä·∫æ·ªÄ·ªÇ·ªÑ·ªÜ√ì√í·ªé√ï·ªå√î·ªê·ªí·ªî·ªñ·ªò∆†·ªö·ªú·ªû·ª†·ª¢√ç√å·ªàƒ®·ªä√ö√ô·ª¶≈®·ª§∆Ø·ª®·ª™·ª¨·ªÆ·ª∞√ù·ª≤·ª∂·ª∏·ª¥ƒê_]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip() # Remove extra whitespace
    return text

def text_preprocess(text):
    text = remove_HTML(text)
    text = convert_unicode(text)
    text = standardize_sentence_typing(text)
    text = normalize_acronyms(text)
    text = word_segmentation(text) # When use PhoBERT
    text = remove_unnecessary_characters(text)
    # return text.lower()
    return text